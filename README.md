下面是**双语版 README.md**（中英对照），已在两种语言顶部注明“由 ChatGPT5 生成 / Generated by ChatGPT5”。你可以直接复制粘贴覆盖仓库里的 `README.md`。

---

# BriLLM v0.15 · README (由 ChatGPT5 生成)

一个以“**边（edge）**”为核心的轻量级语言建模原型：在词（或子词）之间构建有向边 `(u→v)`，为每条边学习独立的仿射变换 `W_e, b_e`，并用**共享未知边参数** `(W_shared, b_shared)` 作为回退；结合可学习门控、位置编码缩放与注意聚合 `a` 实现**稀疏预测 + 稠密表示**的折中。支持**动态批处理**、**自动分词（jieba）**、**断点继承**与**最优权重快照**。

---

## ✨ 特性一览

* **Edge-centric 预测**：仅对“最后一个 token 的出边集合”进行重计算，其余使用共享未知边表示，显著降低计算量。
* **共享回退机制**：未见过的边统一使用 `(W_shared, b_shared)`，保证泛化与稳定。
* **可学习门控与PE缩放**：`gate` 融合历史状态与当前偏置，`pe_scale` 调节PE强度。
* **注意式聚合**：`a`（形如 `[1, L, 1]` 的可学习张量）在时间维做软选择，得到全局表征 `E`。
* **两种推理路径**：单序列 `_forward_one` 与批量 `_forward_batch`，均输出全词表 logits（由边子集“扩散”为全表）。
* **动态批处理与OOM回退**：二分子批，最大化单次吞吐并在 OOM 时自动降档。
* **临时最优快照**：`use_temp_best=True` 自动保存当轮最优权重，支持一键回载。
* **断点继承**：可从 ckpt 继承 `word_to_id / edge2id / d_node / max_seq_len` 等，继续训练或推理。

---

## 📁 目录结构

```
.
├─ architecture/
│  └─ BriLLM_15.py      # 模型实现（BriLLMNodeBias）
├─ main.py              # 示例：数据集、训练、推理
└─ README.md            # 你正在看的文件
```

---

## 🚀 快速开始

### 1) 环境需求

* Python 3.10+
* PyTorch (CUDA 可选)
* `jieba`, `tqdm`, `numpy`

```bash
pip install torch jieba tqdm numpy
```

### 2) 准备数据集

数据集为 **list\[dict]**，每项包含：

```python
{'query': '问题文本（可含<BOS>/<EOS>/<system>等标记）',
 'answer': '答案文本（末尾会在预处理中追加<END>）'}
```

示例见 `main.py`（含 5 条“猫娘系统提示”与多主题多轮样本）。

> 词表包含四个特殊标记：`<BOS>=0, <END>=1, <EOS>=2, <UNK>=3`。
> 分词使用 `jieba.lcut`，未登录词映射至 `<UNK>`。

### 3) 运行训练与推理

```bash
python main.py
```

`main.py` 默认：

* `d_node = 32`, `max_seq_len = 1024`
* `batch = 32`（或 `"auto"` 动态批）
* `lr = 0.002`
* `use_mixed_precision=False`（可切换）
* 训练后调用 `load_best_model()` 载入临时最优并做 `predict` 演示。

---

## 🧠 模型概念与计算流程

### 表征

* `bias_table ∈ R^{V×d}`：每个词（或子词）一个基础偏置向量（可视为词嵌入替代表达）。
* `PE_cache ∈ R^{L×d}`：正余弦位置编码；强度由 `pe_scale` 学习。
* `e_t`：时刻 `t` 的状态，经 `gate` 融合历史与 `h0`（词偏置+PE）。
* `a ∈ R^{1×L×1}`：时间维注意聚合，得到全局表示 `E`。

### 边参数

* `W ∈ R^{|E|×d×d}, b ∈ R^{|E|×d}`：见过的边专属参数。
* `W_shared ∈ R^{d×d}, b_shared ∈ R^{d}`：未知边共享回退。

### 预测

* 取末 token `lastId` 的出边集合 `N(lastId)`（由 `edge2id/edge_freq`）。
* 仅对 `N(lastId)` 重计算得 `y_true`，另算一次 `y_unk`（共享未知边）。
* 构造 `y_all ∈ R^{V×d}`：`true_id` 位置用 `y_true`，其余为 `y_unk`。
* 用 `‖y_all‖_2` 作为打分，得到 `logits → probs`，可配 `top_k/top_p/temperature`。

---

## 🛠️ 代码入口与关键类

### `BriLLMNodeBias(...)`

* **初始化**：若 `inheritance_path` 存在，从 ckpt 恢复；否则基于数据构建词表与边。
* **成员**：`bias_table, W, b, W_shared, b_shared, gate, pe_scale, a, PE_cache, edge2id, edge_freq, eid_table`
* **方法**：`process_data`、`participle`、`auto_create_edge`、`forward`、`train_model`、`predict`、`save/load/load_best_model`

---

## 📊 训练与推理要点

### 批处理

* `batch="auto"`：对 `(q + a[:j])` 构造子批，遇 OOM 自动二分，吞吐最佳。
* `batch=整数`：固定批大小遍历已展开的 `batchDataset/batchY`。
* **无批模式**：逐 token 训练；此模式下不支持 AMP。

### 混合精度

* `use_mixed_precision=True` 时自动选择 `fp16/bf16`（视 CUDA 能力）；也可显式传 `precision_dtype`。
* 需 **CUDA CC ≥ 7.0**；更低请关闭或显式忽略检查。

### 快照与继承

* `use_temp_best=True`：按 `lossItem` 刷新临时最优。
* 可 `load_best_model()` 回载临时最优；或使用持久化 ckpt。

### 超参建议

* `d_node`: 16–64（小数据集）
* `max_seq_len`: ≤ 2048
* `gate≈0.1`、`pe_scale≈0.5` 起步
* `minimum`：边频阈值，>0 以稀疏边集合

---

## 🧪 示例

**训练**

```python
model = BriLLM(d_node=32, l_dataset=(dataset, -1),
               max_seq_len=1024, inheritance_training=True)

model.train_model(
    epochs=10, batch=32, lr=0.002,
    use_mixed_precision=False, use_temp_best=True
)
model.load_best_model()
```

**推理**

```python
for tok, _ in model.predict("<system>扮演一个猫娘<BOS>你喜欢怎么打招呼？", pti=True):
    print(tok, end='')
```

---

## ⚙️ 性能与内存小贴士

* **边裁剪**：用 `minimum` 裁剪低频边，或合并相近 token。
* **共享未知边**：未见边复用 `(W_shared, b_shared)`，减少矩阵乘。
* **动态批**：优先 `"auto"`，在稳定前提下吃满显存。
* **序列长度**：`max_seq_len` 影响显存线性增长，按分布调节。
* **分词质量**：可自定义词典提升边复用度。

---

## 🧾 许可证

研究原型，建议 **MIT License**（按需修改）。

---

## 🙌 致谢

PyTorch、jieba、tqdm 等开源组件，以及对“边驱动语言建模”提出反馈的朋友们。

---

---

# BriLLM v0.15 · README (Generated by ChatGPT5)

A lightweight **edge-centric** language modeling prototype: build directed edges `(u→v)` between tokens/subwords; learn per-edge affine transforms `W_e, b_e`, and fall back to **shared unknown-edge parameters** `(W_shared, b_shared)`. With learnable gating, positional-encoding scaling, and attention-style aggregation `a`, it balances **sparse prediction** and **dense representation**. Supports **dynamic batching**, **jieba tokenization**, **checkpoint inheritance**, and **best-snapshot loading**.

---

## ✨ Highlights

* **Edge-centric inference**: recompute only the outgoing edges of the last token; others use the shared unknown edge—reducing compute.
* **Shared fallback**: unseen edges share `(W_shared, b_shared)` for stability/generalization.
* **Learnable gate & PE scale**: `gate` blends history and local bias; `pe_scale` adjusts PE strength.
* **Attention aggregation**: learnable `a ∈ [1, L, 1]` softly selects over time to form global state `E`.
* **Two inference paths**: `_forward_one` (single) and `_forward_batch` (batched), both returning full-vocab logits (expanded from edge subset).
* **Dynamic batching with OOM backoff**: binary split of sub-batches to maximize throughput while avoiding OOM.
* **Temp best snapshot**: `use_temp_best=True` keeps the best weights of the run for quick reload.
* **Checkpoint inheritance**: resume `word_to_id / edge2id / d_node / max_seq_len` from ckpt for training or inference.

---

## 📁 Structure

```
.
├─ architecture/
│  └─ BriLLM_15.py      # Model (BriLLMNodeBias)
├─ main.py              # Dataset, training, inference demo
└─ README.md
```

---

## 🚀 Quickstart

### 1) Requirements

* Python 3.10+
* PyTorch (CUDA optional)
* `jieba`, `tqdm`, `numpy`

```bash
pip install torch jieba tqdm numpy
```

### 2) Dataset format

A **list\[dict]**:

```python
{'query': 'text (may include <BOS>/<EOS>/<system>)',
 'answer': 'text (an <END> will be appended during preprocessing)'}
```

See `main.py` for examples (5 “system-catgirl” prompts + multi-topic multi-turn samples).

> Special tokens: `<BOS>=0, <END>=1, <EOS>=2, <UNK>=3`.
> Tokenization via `jieba.lcut`; OOV → `<UNK>`.

### 3) Run

```bash
python main.py
```

Defaults in `main.py`:

* `d_node = 32`, `max_seq_len = 1024`
* `batch = 32` (or `"auto"`)
* `lr = 0.002`
* `use_mixed_precision=False`
* After training, `load_best_model()` is called and several `predict` demos run.

---

## 🧠 Model & Computation

### Representations

* `bias_table ∈ R^{V×d}`: per-token base bias (acts like an embedding).
* `PE_cache ∈ R^{L×d}`: sinusoidal PE scaled by `pe_scale`.
* `e_t`: time-step state via `gate` blending history with `h0` (bias+PE).
* `a ∈ R^{1×L×1}`: temporal soft selection to produce global `E`.

### Edge params

* `W ∈ R^{|E|×d×d}, b ∈ R^{|E|×d}`: seen-edge parameters.
* `W_shared ∈ R^{d×d}, b_shared ∈ R^{d}`: fallback for unseen edges.

### Prediction

* Get outgoing set `N(lastId)` from `edge2id/edge_freq`.
* Compute `y_true` over `N(lastId)` and once `y_unk` for the unknown edge.
* Build `y_all ∈ R^{V×d}` by filling `true_id` positions with `y_true`, others with `y_unk`.
* Score with `‖y_all‖_2 → logits → probs`; supports `top_k/top_p/temperature`.

---

## 🛠️ API Surface

* `BriLLMNodeBias(...)`: init (from ckpt or from data).
* `process_data`, `participle`, `auto_create_edge`
* `forward` (routes to `_forward_one` / `_forward_batch`)
* `train_model` (dynamic/fixed batch, AMP optional), `predict`
* `save_model`, `load_model`, `load_best_model`

---

## 📊 Training & Inference Tips

### Batching

* `batch="auto"`: expand `(q + a[:j])` prefixes, auto split on OOM.
* `batch=int`: fixed batch over expanded `batchDataset/batchY`.
* **No-batch mode**: token-by-token; AMP unsupported here.

### Mixed precision

* When `use_mixed_precision=True`, auto-select `fp16/bf16` based on CUDA; or pass `precision_dtype`.
* Requires **CUDA CC ≥ 7.0**.

### Snapshots & Inheritance

* `use_temp_best=True` to keep the best weights by `lossItem`.
* Reload with `load_best_model()` or persist with `save_model()`.

### Hyperparams

* `d_node`: 16–64 for small sets
* `max_seq_len`: ≤ 2048
* Start with `gate≈0.1`, `pe_scale≈0.5`
* Use `minimum` in `auto_create_edge` to prune low-freq edges

---

## 🧪 Examples

**Train**

```python
model = BriLLM(d_node=32, l_dataset=(dataset, -1),
               max_seq_len=1024, inheritance_training=True)

model.train_model(
    epochs=10, batch=32, lr=0.002,
    use_mixed_precision=False, use_temp_best=True
)
model.load_best_model()
```

**Infer**

```python
for tok, _ in model.predict("<system>扮演一个猫娘<BOS>你喜欢怎么打招呼？", pti=True):
    print(tok, end='')
```

---

## ⚙️ Perf & Memory Notes

* **Edge pruning** via `minimum`; or merge similar tokens.
* **Shared unknown edge** greatly reduces matmul count.
* Prefer **`batch="auto"`** to saturate memory safely.
* Tune **`max_seq_len`** per data distribution.
* Consider custom dictionaries to improve token/edge reuse.

---

## 🧾 License

Prototype for research; **MIT** recommended (adjust as needed).

---

## 🙌 Acknowledgements

Thanks to PyTorch, jieba, tqdm, and contributors discussing edge-driven LM ideas.

---
