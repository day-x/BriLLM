ä¸‹é¢æ˜¯**åŒè¯­ç‰ˆ README.md**ï¼ˆä¸­è‹±å¯¹ç…§ï¼‰ï¼Œå·²åœ¨ä¸¤ç§è¯­è¨€é¡¶éƒ¨æ³¨æ˜â€œç”± ChatGPT5 ç”Ÿæˆ / Generated by ChatGPT5â€ã€‚ä½ å¯ä»¥ç›´æ¥å¤åˆ¶ç²˜è´´è¦†ç›–ä»“åº“é‡Œçš„ `README.md`ã€‚

---

# BriLLM v0.15 Â· README (ç”± ChatGPT5 ç”Ÿæˆ)

ä¸€ä¸ªä»¥â€œ**è¾¹ï¼ˆedgeï¼‰**â€ä¸ºæ ¸å¿ƒçš„è½»é‡çº§è¯­è¨€å»ºæ¨¡åŸå‹ï¼šåœ¨è¯ï¼ˆæˆ–å­è¯ï¼‰ä¹‹é—´æ„å»ºæœ‰å‘è¾¹ `(uâ†’v)`ï¼Œä¸ºæ¯æ¡è¾¹å­¦ä¹ ç‹¬ç«‹çš„ä»¿å°„å˜æ¢ `W_e, b_e`ï¼Œå¹¶ç”¨**å…±äº«æœªçŸ¥è¾¹å‚æ•°** `(W_shared, b_shared)` ä½œä¸ºå›é€€ï¼›ç»“åˆå¯å­¦ä¹ é—¨æ§ã€ä½ç½®ç¼–ç ç¼©æ”¾ä¸æ³¨æ„èšåˆ `a` å®ç°**ç¨€ç–é¢„æµ‹ + ç¨ å¯†è¡¨ç¤º**çš„æŠ˜ä¸­ã€‚æ”¯æŒ**åŠ¨æ€æ‰¹å¤„ç†**ã€**è‡ªåŠ¨åˆ†è¯ï¼ˆjiebaï¼‰**ã€**æ–­ç‚¹ç»§æ‰¿**ä¸**æœ€ä¼˜æƒé‡å¿«ç…§**ã€‚

---

## âœ¨ ç‰¹æ€§ä¸€è§ˆ

* **Edge-centric é¢„æµ‹**ï¼šä»…å¯¹â€œæœ€åä¸€ä¸ª token çš„å‡ºè¾¹é›†åˆâ€è¿›è¡Œé‡è®¡ç®—ï¼Œå…¶ä½™ä½¿ç”¨å…±äº«æœªçŸ¥è¾¹è¡¨ç¤ºï¼Œæ˜¾è‘—é™ä½è®¡ç®—é‡ã€‚
* **å…±äº«å›é€€æœºåˆ¶**ï¼šæœªè§è¿‡çš„è¾¹ç»Ÿä¸€ä½¿ç”¨ `(W_shared, b_shared)`ï¼Œä¿è¯æ³›åŒ–ä¸ç¨³å®šã€‚
* **å¯å­¦ä¹ é—¨æ§ä¸PEç¼©æ”¾**ï¼š`gate` èåˆå†å²çŠ¶æ€ä¸å½“å‰åç½®ï¼Œ`pe_scale` è°ƒèŠ‚PEå¼ºåº¦ã€‚
* **æ³¨æ„å¼èšåˆ**ï¼š`a`ï¼ˆå½¢å¦‚ `[1, L, 1]` çš„å¯å­¦ä¹ å¼ é‡ï¼‰åœ¨æ—¶é—´ç»´åšè½¯é€‰æ‹©ï¼Œå¾—åˆ°å…¨å±€è¡¨å¾ `E`ã€‚
* **ä¸¤ç§æ¨ç†è·¯å¾„**ï¼šå•åºåˆ— `_forward_one` ä¸æ‰¹é‡ `_forward_batch`ï¼Œå‡è¾“å‡ºå…¨è¯è¡¨ logitsï¼ˆç”±è¾¹å­é›†â€œæ‰©æ•£â€ä¸ºå…¨è¡¨ï¼‰ã€‚
* **åŠ¨æ€æ‰¹å¤„ç†ä¸OOMå›é€€**ï¼šäºŒåˆ†å­æ‰¹ï¼Œæœ€å¤§åŒ–å•æ¬¡ååå¹¶åœ¨ OOM æ—¶è‡ªåŠ¨é™æ¡£ã€‚
* **ä¸´æ—¶æœ€ä¼˜å¿«ç…§**ï¼š`use_temp_best=True` è‡ªåŠ¨ä¿å­˜å½“è½®æœ€ä¼˜æƒé‡ï¼Œæ”¯æŒä¸€é”®å›è½½ã€‚
* **æ–­ç‚¹ç»§æ‰¿**ï¼šå¯ä» ckpt ç»§æ‰¿ `word_to_id / edge2id / d_node / max_seq_len` ç­‰ï¼Œç»§ç»­è®­ç»ƒæˆ–æ¨ç†ã€‚

---

## ğŸ“ ç›®å½•ç»“æ„

```
.
â”œâ”€ architecture/
â”‚  â””â”€ BriLLM_15.py      # æ¨¡å‹å®ç°ï¼ˆBriLLMNodeBiasï¼‰
â”œâ”€ main.py              # ç¤ºä¾‹ï¼šæ•°æ®é›†ã€è®­ç»ƒã€æ¨ç†
â””â”€ README.md            # ä½ æ­£åœ¨çœ‹çš„æ–‡ä»¶
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1) ç¯å¢ƒéœ€æ±‚

* Python 3.10+
* PyTorch (CUDA å¯é€‰)
* `jieba`, `tqdm`, `numpy`

```bash
pip install torch jieba tqdm numpy
```

### 2) å‡†å¤‡æ•°æ®é›†

æ•°æ®é›†ä¸º **list\[dict]**ï¼Œæ¯é¡¹åŒ…å«ï¼š

```python
{'query': 'é—®é¢˜æ–‡æœ¬ï¼ˆå¯å«<BOS>/<EOS>/<system>ç­‰æ ‡è®°ï¼‰',
 'answer': 'ç­”æ¡ˆæ–‡æœ¬ï¼ˆæœ«å°¾ä¼šåœ¨é¢„å¤„ç†ä¸­è¿½åŠ <END>ï¼‰'}
```

ç¤ºä¾‹è§ `main.py`ï¼ˆå« 5 æ¡â€œçŒ«å¨˜ç³»ç»Ÿæç¤ºâ€ä¸å¤šä¸»é¢˜å¤šè½®æ ·æœ¬ï¼‰ã€‚

> è¯è¡¨åŒ…å«å››ä¸ªç‰¹æ®Šæ ‡è®°ï¼š`<BOS>=0, <END>=1, <EOS>=2, <UNK>=3`ã€‚
> åˆ†è¯ä½¿ç”¨ `jieba.lcut`ï¼Œæœªç™»å½•è¯æ˜ å°„è‡³ `<UNK>`ã€‚

### 3) è¿è¡Œè®­ç»ƒä¸æ¨ç†

```bash
python main.py
```

`main.py` é»˜è®¤ï¼š

* `d_node = 32`, `max_seq_len = 1024`
* `batch = 32`ï¼ˆæˆ– `"auto"` åŠ¨æ€æ‰¹ï¼‰
* `lr = 0.002`
* `use_mixed_precision=False`ï¼ˆå¯åˆ‡æ¢ï¼‰
* è®­ç»ƒåè°ƒç”¨ `load_best_model()` è½½å…¥ä¸´æ—¶æœ€ä¼˜å¹¶åš `predict` æ¼”ç¤ºã€‚

---

## ğŸ§  æ¨¡å‹æ¦‚å¿µä¸è®¡ç®—æµç¨‹

### è¡¨å¾

* `bias_table âˆˆ R^{VÃ—d}`ï¼šæ¯ä¸ªè¯ï¼ˆæˆ–å­è¯ï¼‰ä¸€ä¸ªåŸºç¡€åç½®å‘é‡ï¼ˆå¯è§†ä¸ºè¯åµŒå…¥æ›¿ä»£è¡¨è¾¾ï¼‰ã€‚
* `PE_cache âˆˆ R^{LÃ—d}`ï¼šæ­£ä½™å¼¦ä½ç½®ç¼–ç ï¼›å¼ºåº¦ç”± `pe_scale` å­¦ä¹ ã€‚
* `e_t`ï¼šæ—¶åˆ» `t` çš„çŠ¶æ€ï¼Œç» `gate` èåˆå†å²ä¸ `h0`ï¼ˆè¯åç½®+PEï¼‰ã€‚
* `a âˆˆ R^{1Ã—LÃ—1}`ï¼šæ—¶é—´ç»´æ³¨æ„èšåˆï¼Œå¾—åˆ°å…¨å±€è¡¨ç¤º `E`ã€‚

### è¾¹å‚æ•°

* `W âˆˆ R^{|E|Ã—dÃ—d}, b âˆˆ R^{|E|Ã—d}`ï¼šè§è¿‡çš„è¾¹ä¸“å±å‚æ•°ã€‚
* `W_shared âˆˆ R^{dÃ—d}, b_shared âˆˆ R^{d}`ï¼šæœªçŸ¥è¾¹å…±äº«å›é€€ã€‚

### é¢„æµ‹

* å–æœ« token `lastId` çš„å‡ºè¾¹é›†åˆ `N(lastId)`ï¼ˆç”± `edge2id/edge_freq`ï¼‰ã€‚
* ä»…å¯¹ `N(lastId)` é‡è®¡ç®—å¾— `y_true`ï¼Œå¦ç®—ä¸€æ¬¡ `y_unk`ï¼ˆå…±äº«æœªçŸ¥è¾¹ï¼‰ã€‚
* æ„é€  `y_all âˆˆ R^{VÃ—d}`ï¼š`true_id` ä½ç½®ç”¨ `y_true`ï¼Œå…¶ä½™ä¸º `y_unk`ã€‚
* ç”¨ `â€–y_allâ€–_2` ä½œä¸ºæ‰“åˆ†ï¼Œå¾—åˆ° `logits â†’ probs`ï¼Œå¯é… `top_k/top_p/temperature`ã€‚

---

## ğŸ› ï¸ ä»£ç å…¥å£ä¸å…³é”®ç±»

### `BriLLMNodeBias(...)`

* **åˆå§‹åŒ–**ï¼šè‹¥ `inheritance_path` å­˜åœ¨ï¼Œä» ckpt æ¢å¤ï¼›å¦åˆ™åŸºäºæ•°æ®æ„å»ºè¯è¡¨ä¸è¾¹ã€‚
* **æˆå‘˜**ï¼š`bias_table, W, b, W_shared, b_shared, gate, pe_scale, a, PE_cache, edge2id, edge_freq, eid_table`
* **æ–¹æ³•**ï¼š`process_data`ã€`participle`ã€`auto_create_edge`ã€`forward`ã€`train_model`ã€`predict`ã€`save/load/load_best_model`

---

## ğŸ“Š è®­ç»ƒä¸æ¨ç†è¦ç‚¹

### æ‰¹å¤„ç†

* `batch="auto"`ï¼šå¯¹ `(q + a[:j])` æ„é€ å­æ‰¹ï¼Œé‡ OOM è‡ªåŠ¨äºŒåˆ†ï¼Œååæœ€ä½³ã€‚
* `batch=æ•´æ•°`ï¼šå›ºå®šæ‰¹å¤§å°éå†å·²å±•å¼€çš„ `batchDataset/batchY`ã€‚
* **æ— æ‰¹æ¨¡å¼**ï¼šé€ token è®­ç»ƒï¼›æ­¤æ¨¡å¼ä¸‹ä¸æ”¯æŒ AMPã€‚

### æ··åˆç²¾åº¦

* `use_mixed_precision=True` æ—¶è‡ªåŠ¨é€‰æ‹© `fp16/bf16`ï¼ˆè§† CUDA èƒ½åŠ›ï¼‰ï¼›ä¹Ÿå¯æ˜¾å¼ä¼  `precision_dtype`ã€‚
* éœ€ **CUDA CC â‰¥ 7.0**ï¼›æ›´ä½è¯·å…³é—­æˆ–æ˜¾å¼å¿½ç•¥æ£€æŸ¥ã€‚

### å¿«ç…§ä¸ç»§æ‰¿

* `use_temp_best=True`ï¼šæŒ‰ `lossItem` åˆ·æ–°ä¸´æ—¶æœ€ä¼˜ã€‚
* å¯ `load_best_model()` å›è½½ä¸´æ—¶æœ€ä¼˜ï¼›æˆ–ä½¿ç”¨æŒä¹…åŒ– ckptã€‚

### è¶…å‚å»ºè®®

* `d_node`: 16â€“64ï¼ˆå°æ•°æ®é›†ï¼‰
* `max_seq_len`: â‰¤ 2048
* `gateâ‰ˆ0.1`ã€`pe_scaleâ‰ˆ0.5` èµ·æ­¥
* `minimum`ï¼šè¾¹é¢‘é˜ˆå€¼ï¼Œ>0 ä»¥ç¨€ç–è¾¹é›†åˆ

---

## ğŸ§ª ç¤ºä¾‹

**è®­ç»ƒ**

```python
model = BriLLM(d_node=32, l_dataset=(dataset, -1),
               max_seq_len=1024, inheritance_training=True)

model.train_model(
    epochs=10, batch=32, lr=0.002,
    use_mixed_precision=False, use_temp_best=True
)
model.load_best_model()
```

**æ¨ç†**

```python
for tok, _ in model.predict("<system>æ‰®æ¼”ä¸€ä¸ªçŒ«å¨˜<BOS>ä½ å–œæ¬¢æ€ä¹ˆæ‰“æ‹›å‘¼ï¼Ÿ", pti=True):
    print(tok, end='')
```

---

## âš™ï¸ æ€§èƒ½ä¸å†…å­˜å°è´´å£«

* **è¾¹è£å‰ª**ï¼šç”¨ `minimum` è£å‰ªä½é¢‘è¾¹ï¼Œæˆ–åˆå¹¶ç›¸è¿‘ tokenã€‚
* **å…±äº«æœªçŸ¥è¾¹**ï¼šæœªè§è¾¹å¤ç”¨ `(W_shared, b_shared)`ï¼Œå‡å°‘çŸ©é˜µä¹˜ã€‚
* **åŠ¨æ€æ‰¹**ï¼šä¼˜å…ˆ `"auto"`ï¼Œåœ¨ç¨³å®šå‰æä¸‹åƒæ»¡æ˜¾å­˜ã€‚
* **åºåˆ—é•¿åº¦**ï¼š`max_seq_len` å½±å“æ˜¾å­˜çº¿æ€§å¢é•¿ï¼ŒæŒ‰åˆ†å¸ƒè°ƒèŠ‚ã€‚
* **åˆ†è¯è´¨é‡**ï¼šå¯è‡ªå®šä¹‰è¯å…¸æå‡è¾¹å¤ç”¨åº¦ã€‚

---

## ğŸ§¾ è®¸å¯è¯

ç ”ç©¶åŸå‹ï¼Œå»ºè®® **MIT License**ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰ã€‚

---

## ğŸ™Œ è‡´è°¢

PyTorchã€jiebaã€tqdm ç­‰å¼€æºç»„ä»¶ï¼Œä»¥åŠå¯¹â€œè¾¹é©±åŠ¨è¯­è¨€å»ºæ¨¡â€æå‡ºåé¦ˆçš„æœ‹å‹ä»¬ã€‚

---

---

# BriLLM v0.15 Â· README (Generated by ChatGPT5)

A lightweight **edge-centric** language modeling prototype: build directed edges `(uâ†’v)` between tokens/subwords; learn per-edge affine transforms `W_e, b_e`, and fall back to **shared unknown-edge parameters** `(W_shared, b_shared)`. With learnable gating, positional-encoding scaling, and attention-style aggregation `a`, it balances **sparse prediction** and **dense representation**. Supports **dynamic batching**, **jieba tokenization**, **checkpoint inheritance**, and **best-snapshot loading**.

---

## âœ¨ Highlights

* **Edge-centric inference**: recompute only the outgoing edges of the last token; others use the shared unknown edgeâ€”reducing compute.
* **Shared fallback**: unseen edges share `(W_shared, b_shared)` for stability/generalization.
* **Learnable gate & PE scale**: `gate` blends history and local bias; `pe_scale` adjusts PE strength.
* **Attention aggregation**: learnable `a âˆˆ [1, L, 1]` softly selects over time to form global state `E`.
* **Two inference paths**: `_forward_one` (single) and `_forward_batch` (batched), both returning full-vocab logits (expanded from edge subset).
* **Dynamic batching with OOM backoff**: binary split of sub-batches to maximize throughput while avoiding OOM.
* **Temp best snapshot**: `use_temp_best=True` keeps the best weights of the run for quick reload.
* **Checkpoint inheritance**: resume `word_to_id / edge2id / d_node / max_seq_len` from ckpt for training or inference.

---

## ğŸ“ Structure

```
.
â”œâ”€ architecture/
â”‚  â””â”€ BriLLM_15.py      # Model (BriLLMNodeBias)
â”œâ”€ main.py              # Dataset, training, inference demo
â””â”€ README.md
```

---

## ğŸš€ Quickstart

### 1) Requirements

* Python 3.10+
* PyTorch (CUDA optional)
* `jieba`, `tqdm`, `numpy`

```bash
pip install torch jieba tqdm numpy
```

### 2) Dataset format

A **list\[dict]**:

```python
{'query': 'text (may include <BOS>/<EOS>/<system>)',
 'answer': 'text (an <END> will be appended during preprocessing)'}
```

See `main.py` for examples (5 â€œsystem-catgirlâ€ prompts + multi-topic multi-turn samples).

> Special tokens: `<BOS>=0, <END>=1, <EOS>=2, <UNK>=3`.
> Tokenization via `jieba.lcut`; OOV â†’ `<UNK>`.

### 3) Run

```bash
python main.py
```

Defaults in `main.py`:

* `d_node = 32`, `max_seq_len = 1024`
* `batch = 32` (or `"auto"`)
* `lr = 0.002`
* `use_mixed_precision=False`
* After training, `load_best_model()` is called and several `predict` demos run.

---

## ğŸ§  Model & Computation

### Representations

* `bias_table âˆˆ R^{VÃ—d}`: per-token base bias (acts like an embedding).
* `PE_cache âˆˆ R^{LÃ—d}`: sinusoidal PE scaled by `pe_scale`.
* `e_t`: time-step state via `gate` blending history with `h0` (bias+PE).
* `a âˆˆ R^{1Ã—LÃ—1}`: temporal soft selection to produce global `E`.

### Edge params

* `W âˆˆ R^{|E|Ã—dÃ—d}, b âˆˆ R^{|E|Ã—d}`: seen-edge parameters.
* `W_shared âˆˆ R^{dÃ—d}, b_shared âˆˆ R^{d}`: fallback for unseen edges.

### Prediction

* Get outgoing set `N(lastId)` from `edge2id/edge_freq`.
* Compute `y_true` over `N(lastId)` and once `y_unk` for the unknown edge.
* Build `y_all âˆˆ R^{VÃ—d}` by filling `true_id` positions with `y_true`, others with `y_unk`.
* Score with `â€–y_allâ€–_2 â†’ logits â†’ probs`; supports `top_k/top_p/temperature`.

---

## ğŸ› ï¸ API Surface

* `BriLLMNodeBias(...)`: init (from ckpt or from data).
* `process_data`, `participle`, `auto_create_edge`
* `forward` (routes to `_forward_one` / `_forward_batch`)
* `train_model` (dynamic/fixed batch, AMP optional), `predict`
* `save_model`, `load_model`, `load_best_model`

---

## ğŸ“Š Training & Inference Tips

### Batching

* `batch="auto"`: expand `(q + a[:j])` prefixes, auto split on OOM.
* `batch=int`: fixed batch over expanded `batchDataset/batchY`.
* **No-batch mode**: token-by-token; AMP unsupported here.

### Mixed precision

* When `use_mixed_precision=True`, auto-select `fp16/bf16` based on CUDA; or pass `precision_dtype`.
* Requires **CUDA CC â‰¥ 7.0**.

### Snapshots & Inheritance

* `use_temp_best=True` to keep the best weights by `lossItem`.
* Reload with `load_best_model()` or persist with `save_model()`.

### Hyperparams

* `d_node`: 16â€“64 for small sets
* `max_seq_len`: â‰¤ 2048
* Start with `gateâ‰ˆ0.1`, `pe_scaleâ‰ˆ0.5`
* Use `minimum` in `auto_create_edge` to prune low-freq edges

---

## ğŸ§ª Examples

**Train**

```python
model = BriLLM(d_node=32, l_dataset=(dataset, -1),
               max_seq_len=1024, inheritance_training=True)

model.train_model(
    epochs=10, batch=32, lr=0.002,
    use_mixed_precision=False, use_temp_best=True
)
model.load_best_model()
```

**Infer**

```python
for tok, _ in model.predict("<system>æ‰®æ¼”ä¸€ä¸ªçŒ«å¨˜<BOS>ä½ å–œæ¬¢æ€ä¹ˆæ‰“æ‹›å‘¼ï¼Ÿ", pti=True):
    print(tok, end='')
```

---

## âš™ï¸ Perf & Memory Notes

* **Edge pruning** via `minimum`; or merge similar tokens.
* **Shared unknown edge** greatly reduces matmul count.
* Prefer **`batch="auto"`** to saturate memory safely.
* Tune **`max_seq_len`** per data distribution.
* Consider custom dictionaries to improve token/edge reuse.

---

## ğŸ§¾ License

Prototype for research; **MIT** recommended (adjust as needed).

---

## ğŸ™Œ Acknowledgements

Thanks to PyTorch, jieba, tqdm, and contributors discussing edge-driven LM ideas.

---
